{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJ6rs1O1RqJY50ZRi635ev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsorTheDoctor/visuomotor-robot-policies/blob/main/behavior_transformer/mini_bet_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#miniBET: Behavior Transformer"
      ],
      "metadata": {
        "id": "D3Mx08BWZcgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q torch==1.13.1 torchvision==0.14.1 diffusers==0.18.2 \\\n",
        "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "pygame==2.1.2 pymunk==6.2.1 gym==0.26.2 shapely==1.8.4\n",
        "!git clone https://github.com/PsorTheDoctor/visuomotor-robot-policies.git\n",
        "%cd visuomotor-robot-policies/"
      ],
      "metadata": {
        "id": "m6RnogupAmoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e8ddb7-81f1-45b1-da25-f9c1b54043c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'visuomotor-robot-policies' already exists and is not an empty directory.\n",
            "/content/visuomotor-robot-policies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "import os\n",
        "import numpy as np\n",
        "import gdown\n",
        "import torch\n",
        "import collections\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from utils.env import PushTImageEnv\n",
        "from utils.dataset import PushTImageDataset, normalize_data, unnormalize_data\n",
        "\n",
        "env = PushTImageEnv()\n",
        "env.seed(1000)\n",
        "obs, info = env.reset()\n",
        "action = env.action_space.sample()\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "  print(\"Obs: \", repr(obs))\n",
        "  print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
        "  print(\"Action: \", repr(action))\n",
        "  print(\"Action:   [target_agent_x, target_agent_y]\")"
      ],
      "metadata": {
        "id": "kOPnbiji_2vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "  id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "  gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# pred_horizon = 16\n",
        "# obs_horizon = 2\n",
        "# action_horizon = 8\n",
        "batch_size = 64\n",
        "horizon = 16\n",
        "\n",
        "dataset = PushTImageDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=horizon,\n",
        "    obs_horizon=horizon,\n",
        "    action_horizon=horizon\n",
        ")\n",
        "stats = dataset.stats\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['image'].shape:\", batch['image'].shape)\n",
        "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ],
      "metadata": {
        "id": "1_SHZDPAAcE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb46122a-5544-4fb4-e73b-f74ed7a4cb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch['image'].shape: torch.Size([64, 16, 3, 96, 96])\n",
            "batch['agent_pos'].shape: torch.Size([64, 16, 2])\n",
            "batch['action'].shape torch.Size([64, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs_seq = batch['image'].reshape((batch_size, horizon, 3*96*96))\n",
        "goal_seq = batch['agent_pos']\n",
        "action_seq = batch['action']\n",
        "print('obs_seq.shape:', obs_seq.shape)\n",
        "print('goal_seq.shape:', goal_seq.shape)\n",
        "print('action_seq.shape:', action_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oNi3ysl_ha0",
        "outputId": "93938f62-209c-49e1-da90-d4dd4d512e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs_seq.shape: torch.Size([64, 16, 27648])\n",
            "goal_seq.shape: torch.Size([64, 16, 2])\n",
            "action_seq.shape: torch.Size([64, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/notmahi/miniBET.git\n",
        "%cd miniBET\n",
        "%pip install --upgrade ."
      ],
      "metadata": {
        "id": "wGIj37hQZOyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8f47dc-f5c1-4dc9-8a15-686797ae075c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/visuomotor-robot-policies/miniBET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_xLzY-tZDCi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from behavior_transformer import BehaviorTransformer, GPT, GPTConfig\n",
        "# from examples import dataset\n",
        "\n",
        "conditional = False\n",
        "obs_dim = 27648\n",
        "act_dim = 2\n",
        "goal_dim = obs_dim if conditional else 0\n",
        "K = 32\n",
        "T = 16\n",
        "# batch_size = 256\n",
        "epochs = 5\n",
        "\n",
        "bet = BehaviorTransformer(\n",
        "    obs_dim=obs_dim, act_dim=act_dim, goal_dim=goal_dim,\n",
        "    gpt_model=GPT(GPTConfig(\n",
        "        block_size=144, input_dim=obs_dim, n_layer=6, n_head=8, n_embd=256\n",
        "    )),\n",
        "    n_clusters=K, kmeans_fit_steps=5\n",
        ")\n",
        "optimizer = bet.configure_optimizers(\n",
        "    weight_decay=2e-4, learning_rate=1e-5, betas=[0.9, 0.999]\n",
        ")\n",
        "with tqdm(range(epochs), desc='Epoch') as tglobal:\n",
        "  for epoch_idx in tglobal:\n",
        "    epoch_loss = list()\n",
        "    with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "      for nbatch in tepoch:\n",
        "        print(nbatch['image'].shape)\n",
        "        obs_seq = nbatch['image'].reshape((batch_size, horizon, 3*96*96))[:, :horizon]\n",
        "        goal_seq = nbatch['agent_pos'][:, :horizon]\n",
        "        action_seq = nbatch['action']\n",
        "\n",
        "        train_action, train_loss, train_loss_dict = bet(obs_seq, goal_seq, action_seq)\n",
        "\n",
        "        # # Action inference\n",
        "        # eval_action, eval_loss, eval_loss_dict = bet(obs_seq, goal_seq, None)\n",
        "        # print('Eval loss:', eval_loss)\n",
        "\n",
        "        epoch_loss.append(float(train_loss))\n",
        "        tepoch.set_postfix(loss=train_loss)\n",
        "    tglobal.set_postfix(loss=np.mean(epoch_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchviz torchview"
      ],
      "metadata": {
        "id": "bpuhm9T1hRYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(train_action, params=dict(bet.named_parameters())).render(format='png')"
      ],
      "metadata": {
        "id": "4UUVDRGKhi0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "\n",
        "model_graph = draw_graph(bet(), input_size=(batch_size, T, obs_dim), expand_nested=False)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "fui1HXOWk7tj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}